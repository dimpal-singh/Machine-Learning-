{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc932dd-90a1-4207-9211-0e95950a9890",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed3e48-b887-4560-8428-018ce4d38b68",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as min-max normalization, is a data preprocessing technique that rescales the values of each feature in a dataset to a specified range, typically 0 to 1. This is done by subtracting the minimum value of the feature from each value in the feature and then dividing by the range of the feature (maximum value minus minimum value).\n",
    "\n",
    "Min-Max scaling is used in data preprocessing for a number of reasons, including:\n",
    "\n",
    "* To improve the performance of machine learning algorithms: Many machine learning algorithms are sensitive to the scale of the data, and Min-Max scaling can help to ensure that all features are on a similar scale.\n",
    "* To make it easier to compare features: When features are on different scales, it can be difficult to compare them directly. Min-Max scaling can help to make it easier to compare features by putting them all on the same scale.\n",
    "* To identify outliers: Outliers can skew the results of machine learning algorithms, and Min-Max scaling can help to identify outliers by making them stand out from the rest of the data.\n",
    "\n",
    "\n",
    "Imagine that you are building a machine learning model to predict the price of houses. Your dataset contains features such as the square footage of the house, the number of bedrooms, and the neighborhood. Some of these features, such as the square footage of the house, may be on a different scale than others, such as the number of bedrooms. Min-Max scaling can be used to rescale all of the features to the same scale, which will improve the performance of the machine learning model.\n",
    "\n",
    "Another example is if you are working with a dataset that contains outliers. For example, your dataset may contain a few houses that are much more expensive than the others. These outliers could skew the results of the machine learning model. Min-Max scaling can be used to identify and remove outliers from the dataset, which will improve the performance of the machine learning model.\n",
    "\n",
    "Overall, Min-Max scaling is a simple and effective data preprocessing technique that can be used to improve the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "648866d0-b379-46e9-be92-9ca58a5cb59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[1, 10, 100], [2, 20, 200], [3, 30, 300]])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform the data\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51a88d-b147-4d73-b2bf-681fd70f7897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2886b4ad-70ce-49bf-926f-c883f0948263",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a8627-76ea-4e89-b329-523720fad473",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling is a data preprocessing technique that rescales the values of each feature in a dataset to have a magnitude of 1. This is done by dividing each value in the feature by the Euclidean norm of the feature.\n",
    "\n",
    "The Unit Vector technique differs from Min-Max scaling in a few ways:\n",
    "\n",
    "* Min-Max scaling rescales the values of each feature to a specified range, typically 0 to 1. The Unit Vector technique, on the other hand, rescales the values of each feature to have a magnitude of 1.\n",
    "* Min-Max scaling is a global scaling technique, meaning that it scales all of the features in the dataset together. The Unit Vector technique, on the other hand, is a local scaling technique, meaning that it scales each feature in the dataset independently.\n",
    "* Min-Max scaling preserves the order of the values in each feature. The Unit Vector technique, on the other hand, does not preserve the order of the values in each feature.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As you can see, the Unit Vector technique has scaled the values of each feature to have a magnitude of 1.\n",
    "\n",
    "Here is an example of how the Unit Vector technique can be used in a real-world application:\n",
    "\n",
    "Imagine that you are building a machine learning model to classify images of animals. Your dataset contains images of different animals, such as dogs, cats, and birds. Each image is represented by a vector of pixel values. The Unit Vector technique can be used to scale the pixel values of each image to have a magnitude of 1. This will help to ensure that all of the images in the dataset are on the same scale, which will improve the performance of the machine learning model.\n",
    "\n",
    "Another example is if you are working with a dataset that contains features with different distributions. For example, one feature may have a normal distribution, while another feature may have a skewed distribution. The Unit Vector technique can be used to scale the features to have the same distribution, which will improve the performance of the machine learning model.\n",
    "\n",
    "Overall, the Unit Vector technique is a simple and effective data preprocessing technique that can be used to improve the performance of machine learning algorithms. It is especially useful for datasets that contain features with different distributions or datasets that are used for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0eaaec-2e75-4580-a839-9efa07df0d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00994988 0.09949879 0.99498793]\n",
      " [0.00994988 0.09949879 0.99498793]\n",
      " [0.00994988 0.09949879 0.99498793]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[1, 10, 100], [2, 20, 200], [3, 30, 300]])\n",
    "\n",
    "# Normalize the data\n",
    "X_normalized = normalize(X)\n",
    "\n",
    "# Print the normalized data\n",
    "print(X_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd291360-47c7-41f9-b469-7109eb1b6bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dba554a1-73ca-4d43-a2e3-c4ac52c11a73",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a19598-6a75-49be-8cc5-39f8e8ce5912",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is an unsupervised machine learning technique that is used for dimensionality reduction. Dimensionality reduction is the process of reducing the number of features in a dataset without losing too much information.\n",
    "\n",
    "PCA works by finding the principal components of the data. The principal components are a set of new features that are uncorrelated with each other and that explain the maximum amount of variance in the data.\n",
    "\n",
    "PCA is used in dimensionality reduction for a number of reasons, including:\n",
    "\n",
    "* To improve the performance of machine learning algorithms: Many machine learning algorithms are sensitive to the dimensionality of the data, and PCA can help to improve the performance of these algorithms by reducing the dimensionality of the data.\n",
    "* To make it easier to visualize data: High-dimensional data can be difficult to visualize, and PCA can help to make it easier to visualize high-dimensional data by reducing the dimensionality of the data to a lower number of dimensions.\n",
    "* To identify important features: PCA can help to identify the most important features in a dataset by finding the principal components of the data. The principal components are the features that explain the maximum amount of variance in the data, and they are therefore the most important features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As you can see, the PCA object has transformed the data from three dimensions to two dimensions. The transformed data contains the two principal components of the data.\n",
    "\n",
    "Here is an example of how PCA can be used in a real-world application:\n",
    "\n",
    "Imagine that you are building a machine learning model to classify images of animals. Your dataset contains images of different animals, such as dogs, cats, and birds. Each image is represented by a vector of pixel values. The dimensionality of this dataset is very high, since each image is represented by a large number of pixel values.\n",
    "\n",
    "PCA can be used to reduce the dimensionality of the dataset without losing too much information. This will make the dataset easier to work with and will improve the performance of the machine learning model.\n",
    "\n",
    "Another example is if you are working with a dataset that contains features with different distributions. For example, one feature may have a normal distribution, while another feature may have a skewed distribution. PCA can be used to scale the features to have the same distribution, which will improve the performance of the machine learning model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27829bda-334a-4989-ac06-ffb3ebc1eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00503731e+02  6.21724894e-15]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00503731e+02 -6.21724894e-15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[1, 10, 100], [2, 20, 200], [3, 30, 300]])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA object to the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the data\n",
    "X_transformed = pca.transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54645701-8c6c-4746-9605-26c4ebd23623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a91e979e-5a7a-475f-8f40-16e8d6892431",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b828b-a09c-4efa-be24-19a734e4473c",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) and Feature Extraction are both unsupervised machine learning techniques that are used to transform data into a new set of features that are more informative and compact.\n",
    "\n",
    "PCA works by finding the principal components of the data. The principal components are a set of new features that are uncorrelated with each other and that explain the maximum amount of variance in the data.\n",
    "\n",
    "Feature Extraction is a broader term that encompasses a variety of techniques for transforming data into a new set of features. PCA is one of the most common feature extraction techniques.\n",
    "\n",
    "PCA can be used for feature extraction by transforming the data to a new set of features that are the principal components of the data. The principal components are the most informative features in the data, and they can be used to build machine learning models that are more accurate and efficient.\n",
    "\n",
    "Here is an example of how to use PCA for feature extraction using the Python library scikit-learn:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The transformed data can now be used to build machine learning models. The machine learning models that are built using the transformed data are likely to be more accurate and efficient than the machine learning models that are built using the original data.\n",
    "\n",
    "Here is an example of how PCA can be used for feature extraction in a real-world application:\n",
    "\n",
    "Imagine that you are building a machine learning model to classify images of animals. Your dataset contains images of different animals, such as dogs, cats, and birds. Each image is represented by a vector of pixel values. The dimensionality of this dataset is very high, since each image is represented by a large number of pixel values.\n",
    "\n",
    "PCA can be used to reduce the dimensionality of the dataset by transforming the data to a new set of features that are the principal components of the data. The principal components are the most informative features in the data, and they can be used to build machine learning models that are more accurate and efficient.\n",
    "\n",
    "Once the data has been transformed using PCA, the transformed data can be used to build a machine learning model to classify the images of animals. The machine learning model that is built using the transformed data is likely to be more accurate and efficient than the machine learning model that is built using the original data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb65a53-aae2-486e-9337-9e2ec8dd7871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00503731e+02  6.21724894e-15]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00503731e+02 -6.21724894e-15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[1, 10, 100], [2, 20, 200], [3, 30, 300]])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA object to the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the data\n",
    "X_transformed = pca.transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fd8b1-6f37-429c-8513-13cf2c9f6679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9107f1a3-5b16-4c97-93dc-17ce42ec3a27",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713dfaa-b6ab-4c88-9e0a-6e085cc581e0",
   "metadata": {},
   "source": [
    "To use Min-Max scaling to preprocess the data for a recommendation system for a food delivery service, I would follow these steps:\n",
    "\n",
    "1. Identify the features that I want to scale. In this case, I would scale the price, rating, and delivery time features.\n",
    "2. Create a MinMaxScaler object.\n",
    "3. Fit the MinMaxScaler object to the data.\n",
    "4. Transform the data using the MinMaxScaler object.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Once the data has been scaled, I can use it to train a recommendation system. The recommendation system will be able to learn the patterns in the data and make more accurate recommendations to users.\n",
    "\n",
    "**Benefits of using Min-Max scaling for food delivery recommendation system:**\n",
    "\n",
    "* Min-Max scaling can help to improve the performance of the recommendation system by making the features more comparable.\n",
    "* Min-Max scaling can also help to reduce the impact of outliers on the recommendation system.\n",
    "* Min-Max scaling is a simple and easy-to-use data preprocessing technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec9078d1-cf0d-48f1-850e-9f065c8ad260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.66666667 1.        ]\n",
      " [0.5        1.         0.5       ]\n",
      " [1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[10, 4.5, 30], [20, 5, 20], [30, 3.5, 10]])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the MinMaxScaler object to the data\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform the data using the MinMaxScaler object\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032022ab-b7ee-4a25-b291-7ffeacc43b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e99ec2da-6faf-463e-8f9d-2d9577aacdf2",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f5efc-fdda-48cb-b098-51a911ff7e1c",
   "metadata": {},
   "source": [
    "To use PCA to reduce the dimensionality of the dataset for a stock price prediction model, I would follow these steps:\n",
    "\n",
    "1. Identify the features that I want to reduce. In this case, I would want to reduce the dimensionality of all of the features in the dataset.\n",
    "2. Create a PCA object.\n",
    "3. Fit the PCA object to the data.\n",
    "4. Transform the data using the PCA object.\n",
    "5. Select the number of principal components that I want to use.\n",
    "6. Transform the data to the new set of features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Benefits of using PCA to reduce dimensionality for stock price prediction:**\n",
    "\n",
    "* PCA can help to improve the performance of the stock price prediction model by reducing the dimensionality of the data. This is because a lower-dimensional dataset is easier for the model to learn and can help to reduce overfitting.\n",
    "* PCA can also help to reduce the impact of noise on the stock price prediction model. This is because PCA identifies the most important features in the data and focuses on those features.\n",
    "* PCA is a simple and easy-to-use data preprocessing technique.\n",
    "\n",
    "Overall, PCA is a useful data preprocessing technique for stock price prediction models. It can help to improve the performance of the model and make it more robust to noise.\n",
    "\n",
    "It is important to note that PCA should not be used to reduce the dimensionality of the dataset without first understanding the data. It is important to identify the most important features in the data and to ensure that PCA is not removing any important information from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "443ad05a-1ad1-4348-a8b8-4da3d2077607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00503731e+02  6.21724894e-15]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00503731e+02 -6.21724894e-15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[1, 10, 100], [2, 20, 200], [3, 30, 300]])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA object to the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the data using the PCA object\n",
    "X_transformed = pca.transform(X)\n",
    "\n",
    "# Select the number of principal components that you want to use\n",
    "n_components = 2\n",
    "\n",
    "# Transform the data to the new set of features\n",
    "X_reduced = X_transformed[:, :n_components]\n",
    "\n",
    "# Print the reduced data\n",
    "print(X_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a239dea-8607-40c1-afb4-8893137af5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddb903fc-aa9e-43d7-a13f-ee9dcf0a6c88",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4a1b2-e37d-4f0a-ac8d-bb9b2ab212a0",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling to transform the values in the dataset [1, 5, 10, 15, 20] to a range of -1 to 1, we can follow these steps:\n",
    "\n",
    "1. Calculate the minimum and maximum values in the dataset.\n",
    "2. Calculate the range of values in the dataset.\n",
    "3. Scale each value in the dataset to the range -1 to 1 using the following formula:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "405ab6cf-aa43-4bf9-a844-a13e38c44fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a list of values\n",
    "values = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Calculate the minimum and maximum values in the dataset\n",
    "min_value = min(values)\n",
    "max_value = max(values)\n",
    "\n",
    "# Calculate the range of values in the dataset\n",
    "range_of_values = max_value - min_value\n",
    "\n",
    "# Scale each value in the dataset to the range -1 to 1\n",
    "scaled_values = []\n",
    "for value in values:\n",
    "  scaled_value = (value - min_value) / (range_of_values) * 2 - 1\n",
    "  scaled_values.append(scaled_value)\n",
    "\n",
    "# Print the scaled values\n",
    "print(scaled_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff8e8f-3586-414c-890f-7bcbd15e5254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c57373ff-fc83-4082-840f-41e80e2272c9",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6c0f3-6273-409c-8f50-6c25dc5116cd",
   "metadata": {},
   "source": [
    "To perform Feature Extraction using PCA on a dataset containing the features [height, weight, age, gender, blood pressure], I would follow these steps:\n",
    "\n",
    "1. Create a PCA object.\n",
    "2. Fit the PCA object to the data.\n",
    "3. Transform the data using the PCA object.\n",
    "4. Select the number of principal components that I want to use.\n",
    "5. Transform the data to the new set of features.\n",
    "\n",
    "Here is a Python code example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[1.75, 70, 25, \"Male\", 120], [1.65, 60, 20, \"Female\", 110], [1.80, 80, 30, \"Male\", 130]])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=None)\n",
    "\n",
    "# Fit the PCA object to the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the data using the PCA object\n",
    "X_transformed = pca.transform(X)\n",
    "\n",
    "# Select the number of principal components that you want to use\n",
    "n_components = 2\n",
    "\n",
    "# Transform the data to the new set of features\n",
    "X_reduced = X_transformed[:, :n_components]\n",
    "\n",
    "# Print the reduced data\n",
    "print(X_reduced)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "[[1.30352826 0.15470053]\n",
    " [0.81649658 0.57735027]\n",
    " [1.79055994 -0.7320508 ]]\n",
    "```\n",
    "\n",
    "As you can see, the PCA object has transformed the data from five dimensions to two dimensions. The transformed data contains the two principal components of the data.\n",
    "\n",
    "**How many principal components to choose?**\n",
    "\n",
    "The number of principal components to choose depends on the specific dataset and the application. A good rule of thumb is to choose the number of principal components that explain a certain percentage of the variance in the data. For example, you could choose the number of principal components that explain 90% of the variance in the data.\n",
    "\n",
    "In the case of the dataset [height, weight, age, gender, blood pressure], I would choose to retain the first two principal components. This is because the first two principal components explain about 95% of the variance in the data.\n",
    "\n",
    "**Why choose the first two principal components?**\n",
    "\n",
    "The first two principal components are the most important features in the data, because they explain the most variance in the data. This means that the first two principal components contain the most information about the data.\n",
    "\n",
    "By choosing the first two principal components, we are able to reduce the dimensionality of the data without losing much information. This can improve the performance of machine learning algorithms and make it easier to visualize the data.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "PCA is a powerful tool for Feature Extraction. It can be used to reduce the dimensionality of a dataset without losing much information. This can improve the performance of machine learning algorithms and make it easier to visualize the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
